# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13dp5XdXoFiYJVcOQzXZ9RLPn4nPw9vib

# Final project of EEE4170
## Classify EMNIST(by merge) Using pytorch 
### 20181485 고재현
"""

import torch
import torchvision
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from torch.optim import lr_scheduler
import torch.optim as optim
import numpy as np
from itertools import islice
import math
import time
import torch.nn.functional as F

student_id = '20181485'


def calc_distribution(dataset):
    x = np.concatenate([np.asarray(dataset[i][0]) for i in range(len(dataset))])
    print(x.shape)
    train_mean = np.mean(x)  # , axis=(0, 1,2))
    train_std = np.std(x)  # axis=(0,1,2))
    print(train_mean / 255, train_std / 255)


### train 또는 test dataset에 대하여, num의 수만큼 subplot을 보여주는 함수입니다.
def image_show(dataset, num):
    fig = plt.figure(figsize=(30, 30))

    for i in range(num):
        plt.subplot(1, num, i + 1)
        plt.imshow(dataset[i][0].squeeze())
        plt.title(dataset[i][1])


class TypeData(Dataset):
    '''
  ### Digit일 경우 label로 0을, ###
  ### Letter일 경우 label로 1을 ###
  ### return하는 class입니다. ###
  사용 예시:
  train_data = TypeData(train=True)
  test_data = TypeData(train=False)
    '''

    def __init__(self, train, datatype):
        super(TypeData, self).__init__()
        self.digit = 10
        self.letter = 46
        self.train = train
        self.datatype = datatype

        self.data = torchvision.datasets.EMNIST(root='./',
                                                split=self.datatype,
                                                train=self.train,
                                                transform=transform,  # transforms.ToTensor(),
                                                download=True)

    def __getitem__(self, index):
        if self.datatype == 'bymerge':
            if self.data[index][1] < self.digit:
                label = 0.
            else:
                label = 1.

        elif self.datatype == 'letters':
            label = 1.

        else:  # digits
            label = 0.

        return self.data[index][0], torch.tensor(self.data[index][1]), torch.tensor(label)

    def __len__(self):
        return len(self.data)


transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=(0), std=(0.0013))
    # bymerge-train dataset을 이용해서 구한 mean과 std. mean은 0에 가깝지만 std가 1보다 훨씬 작으니 정규화해주자.
])

train_data = TypeData(train=True, datatype='bymerge')  # model 1 train
digits_train = TypeData(train=True, datatype='letters')
letters_train = TypeData(train=True, datatype='digits')
test_data = TypeData(train=False, datatype='bymerge')  # model 1 test
l_test_data = TypeData(train=False, datatype='letters')  # model 1 test
d_test_data = TypeData(train=False, datatype='digits')  # model 1 test

# print((train_data.data))

# calc_distribution(train_data.data)

# set hyperparameter
batch_size = 128
learning_rate = 0.01
num_epoch = 10

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)
d_train_loader = DataLoader(digits_train, batch_size=batch_size, shuffle=True, drop_last=True)
l_train_loader = DataLoader(letters_train, batch_size=batch_size, shuffle=True, drop_last=True)
dloaders=[l_train_loader] #d_train_loader,
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, drop_last=True)
d_test_loader = DataLoader(d_test_data, batch_size=batch_size, shuffle=False, drop_last=True)
l_test_loader = DataLoader(l_test_data, batch_size=batch_size, shuffle=False, drop_last=True)

class m2CNN(nn.Module):
    def __init__(self):
        super(m2CNN, self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.fc_layer1 = nn.Sequential(
            nn.Linear(64 * 7 * 7, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Linear(64, 47),
            nn.BatchNorm1d(47),
            nn.ReLU(),

        )


    def forward(self, x):
        out = self.layer(x)
        out = out.view(batch_size, -1)
        out = self.fc_layer1(out)
        return out


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.layer = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding=1),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2, 2),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2)
        )
        self.fc_layer1 = nn.Sequential(
            nn.Linear(64 * 7 * 7, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Linear(64, 47),
            nn.BatchNorm1d(47),
            nn.ReLU(),

        )
        self.threshold_layer = nn.Sequential(
            nn.Linear(47, 2),
            nn.ReLU(),
        )

    # def threshold(self, out):
    #     if out[0] < 10:
    #         label = 0.
    #     else:
    #         label = 1.

    def forward(self, x):
        out = self.layer(x)
        out = out.view(batch_size, -1)
        out = self.fc_layer1(out)
        label = self.threshold_layer(out)
        return out, label

class bin_classifier(nn.Module):
    def __init__(self):
        super(bin_classifier, self).__init__()

        self.threshold_layer = nn.Sequential(
            nn.Linear(47, 2),
            nn.ReLU(),)

    def forward(self, x):
        label = self.threshold_layer(x)
        return label

Half_width = 128
layer_width = 128


start = time.time()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
d_model = m2CNN().to(device)
l_model = m2CNN().to(device)
b_model = bin_classifier().to(device)
loss_func = nn.CrossEntropyLoss().to(device)
b_optimizer = optim.Adam(b_model.parameters(), lr=learning_rate)
d_optimizer = optim.Adam(d_model.parameters(), lr=learning_rate)
l_optimizer = optim.Adam(l_model.parameters(), lr=learning_rate)
b_scheduler = lr_scheduler.ReduceLROnPlateau(b_optimizer, factor=0.1, patience=10, mode='min', eps=1e-08)
d_scheduler = lr_scheduler.ReduceLROnPlateau(d_optimizer, factor=0.1, patience=10, mode='min', eps=1e-08)
l_scheduler = lr_scheduler.ReduceLROnPlateau(l_optimizer, factor=0.1, patience=10, mode='min', eps=1e-08)


for i in range(num_epoch):

    for n, [image, label1, label2] in islice(enumerate(train_loader),0,1024):
        x = image.to(device)
        y1_ = label1.to(device=device, dtype=torch.int64)
        y2_ = label2.to(device=device, dtype=torch.int64)
        if torch.sum(label2)/batch_size < torch.tensor(0.5):
            d_optimizer.zero_grad()
            d_output = d_model.forward(x)
            d_loss = (loss_func(d_output, y1_))
            d_loss.backward()
            d_optimizer.step()
        else:
            l_optimizer.zero_grad()
            l_output = l_model.forward(x)
            l_loss = (loss_func(l_output, y1_))
            l_loss.backward()
            l_optimizer.step()
        d_model.eval()
        l_model.eval()
        m2_out = torch.sum(torch.stack([d_model.forward(x),l_model.forward(x)]), dim=0)
        m2_loss = loss_func(m2_out, y1_)
        b_optimizer.zero_grad()
        label = b_model.forward(m2_out)
        b_loss = (loss_func(label, y2_))
        b_loss.backward()
        b_optimizer.step()
        d_model.train()
        l_model.train()
    d_scheduler.step(d_loss)
    l_scheduler.step(l_loss)
    b_scheduler.step(b_loss)

    print('EPOCH: {}, m2_Loss: {}, Loss: {}, d_LR: {}, l_LR: {}, b_LR: {}'.format(i+1, m2_loss.item(), b_loss.item(),
                                                           d_scheduler.optimizer.state_dict()['param_groups'][0][
                                                               'lr'], l_scheduler.optimizer.state_dict()[
                                                                                      'param_groups'][0]['lr'],
                                                                                  b_scheduler.optimizer.state_dict()['param_groups'][0]['lr']))
torch.save(d_model.state_dict(), './Train_Results/' + 'd_model_' + student_id + '.pth')
torch.save(l_model.state_dict(), './Train_Results/' + 'l_model_' + student_id + '.pth')
torch.save(b_model.state_dict(), './Train_Results/' + 'b_model_' + student_id + '.pth')
print(time.time() - start)

d_test_model = m2CNN().to(device)
l_test_model = m2CNN().to(device)
b_test_model = bin_classifier().to(device)
d_checkpoint = torch.load('./Train_Results/' + 'd_model_' + student_id + '.pth', map_location=device)
l_checkpoint = torch.load('./Train_Results/' + 'l_model_' + student_id + '.pth', map_location=device)
b_checkpoint = torch.load('./Train_Results/' + 'b_model_' + student_id + '.pth', map_location=device)
d_test_model.load_state_dict(d_checkpoint)
l_test_model.load_state_dict(l_checkpoint)
b_test_model.load_state_dict(b_checkpoint)

correct1 = 0
correct2 = 0
total1 = 0
total2 = 0
d_test_model.eval()
l_test_model.eval()
b_test_model.eval()
with torch.no_grad():
    for image, label1, label2 in test_loader:
        x = image.to(device)
        y1_ = label1.to(device)
        y2_ = label2.to(device)

        d_output = d_test_model.forward(x)
        l_output = l_test_model.forward(x)
        output1 = torch.sum(torch.stack([d_output,l_output]), dim=0)
        output2 = b_test_model.forward(output1)
        _, output1_index = torch.max(output1, 1)
        _, output2_index = torch.max(output2, 1)
        total1 += label1.size(0)
        total2 += label2.size(0)
        correct1 += (output1_index == y1_).sum().float()
        correct2 += (output2_index == y2_).sum().float()
    print("model 2 Accuracy of Test Data: {}%".format(100.0 * correct1 / total1))
    print("model 1 Accuracy of Test Data: {}%".format(100.0 * correct2 / total2))
##45%